{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face's `transformers` library includes tokenizers that can convert text data into a format suitable for machine learning models, particularly models that require tensor input like those in PyTorch. When you use the argument `return_tensors=\"pt\"`, the tokenizer ensures the output is a PyTorch tensor. However, when you access individual items from a dataset, especially when using the `datasets` library from Hugging Face, the behavior changes slightly.\n",
    "\n",
    "### Why Hugging Face Returns Lists for Individual Elements\n",
    "\n",
    "When you subset a dataset to access individual elements, the `datasets` library typically returns data as Python lists or dictionaries containing lists rather than tensors. This occurs for several reasons:\n",
    "\n",
    "1. **Versatility**: Lists are a more general data structure and are not tied to any specific backend like PyTorch or TensorFlow. This makes the data more accessible for different operations, such as data manipulation or inspection without requiring tensor operations.\n",
    "\n",
    "2. **Simplicity**: Lists are easier to handle for many standard Python operations, including simple modifications and printing, which might be more complex with tensors.\n",
    "\n",
    "3. **Data Inspection**: Returning data in list format when accessing single elements makes it easier to examine specific entries, which is often necessary during data exploration and debugging.\n",
    "\n",
    "### Example Scenario\n",
    "\n",
    "Suppose you have a dataset of text sentences that you tokenize with Hugging Face's tokenizer set to return PyTorch tensors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tddnth\\workspace\\kaggle\\competitions\\learning-agency-lab-automated-essay-scoring-2\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d58934f50a4dd6a49e411191f8c5c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "sample_data = [\"Hello, world!\", \"Hugging Face is cool!\"]\n",
    "dataset = Dataset.from_dict({\"sentence\": sample_data})\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_data = dataset.map(\n",
    "    lambda x: tokenizer(x['sentence'], \n",
    "                        return_tensors=\"pt\", \n",
    "                        padding=True, \n",
    "                        truncation=True), \n",
    "    batched=False\n",
    ")\n",
    "\n",
    "# Access the first tokenized input\n",
    "first_input = tokenized_data[0]\n",
    "\n",
    "# Examine the types of the data returned\n",
    "print(type(first_input['input_ids']))\n",
    "print(type(first_input['attention_mask']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this example, despite the tokenizer being set to return PyTorch tensors (`return_tensors=\"pt\"`), when you access an individual item (`tokenized_data[0]`), the output will be in list format. This is how the `datasets` library processes and returns individual dataset entries.\n",
    "\n",
    "### Handling and Converting Data\n",
    "\n",
    "If you need to ensure that the data remains as tensors, especially when subsetting, you should handle this conversion explicitly after accessing the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Convert list to PyTorch tensor if needed\n",
    "input_ids = torch.tensor(first_input['input_ids'])\n",
    "attention_mask = torch.tensor(first_input['attention_mask'])\n",
    "\n",
    "print(input_ids.dtype)  # This will show torch.int64 (torch.long), for example\n",
    "print(attention_mask.dtype)  # Similarly, this will show tensor data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "By converting the lists back to tensors, you can ensure that the data is in the correct format for feeding into a neural network for training or inference. This step is crucial in maintaining the tensor format through all stages of data handling and model processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
